{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Convolutional Neural Network (CNN), and how does it differ from traditional fully connected neural networks in terms of architecture and performance on image data?**\n",
        "\n",
        "**What is a CNN?**\n",
        "\n",
        "* A CNN is a type of neural network designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using:\n",
        "\n",
        "**Convolutional layers**\n",
        "\n",
        "**Pooling (subsampling) layers**\n",
        "\n",
        "**Fully connected layers (at the end, for classification or regression)**\n",
        "\n",
        "**Key Architectural Differences Between CNN and FCN**\n",
        "\n",
        "**Feature**.........................................\t**Fully Connected Network (FCN)**\t......................................**Convolutional Neural Network (CNN)**\n",
        "\n",
        "\n",
        "**Input Handling**............................\tInput is flattened into a 1D vector (e.g., a 28√ó28 image becomes a 784-dimensional vector).......................................\tInput retains its 2D or 3D spatial structure (e.g., height √ó width √ó channels)\n",
        "\n",
        "**Layer Connections**......................................\tEvery neuron is connected to every neuron in the previous layer..........................\tUses local connections (filters) over small regions of the input\n",
        "\n",
        "**Weight Sharing**\tEach connection has a unique weight.........................\tFilters (kernels) are shared across the input image, drastically reducing parameters\n",
        "\n",
        "**Parameter Efficiency**.............................\tLarge number of parameters, prone to overfitting................................\tFewer parameters, more scalable to larger inputs\n",
        "\n",
        "**Translation Invariance**\tPoor (must learn each variation separately)....................................\tGood due to convolution and pooling operations capturing local patterns\n",
        "\n",
        "**Spatial Hierarchy**...........................\tDoes not preserve spatial relationships............................\tPreserves and exploits spatial structure of the input\n"
      ],
      "metadata": {
        "id": "3cGASUSQyKx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Discuss the architecture of LeNet-5 and explain how it laid the foundation for modern deep learning models in computer vision. Include references to its original research paper.**\n",
        "\n",
        "\n",
        "\n",
        "*  The LeNet-5 architecture, proposed by Yann LeCun et al. in 1998, is one of the earliest and most influential Convolutional Neural Networks (CNNs). It was originally developed for handwritten digit recognition, particularly the MNIST dataset (digits 0‚Äì9), and its architecture introduced core design principles that underpin modern deep learning models in computer vision today.\n",
        "\n",
        "**LeNet-5 Architecture Breakdown**\n",
        "\n",
        "* LeNet-5 is a 7-layer network (excluding the input), composed of:\n",
        "\n",
        "**1. Input Layer**\n",
        "\n",
        "* Input: 32√ó32 grayscale image\n",
        "\n",
        "* MNIST images (28√ó28) were zero-padded to 32√ó32 to preserve spatial resolution after convolutions.\n",
        "\n",
        "**2. C1 ‚Äì First Convolutional Layer**\n",
        "\n",
        "* Type: Convolution\n",
        "\n",
        "* Filter size: 5√ó5\n",
        "\n",
        "* Number of filters: 6\n",
        "\n",
        "* Output size: 28√ó28√ó6\n",
        "\n",
        "* Activation: Tanh (used instead of ReLU)\n",
        "\n",
        "* Trainable parameters: 6√ó(5√ó5 + 1) = 156\n",
        "\n",
        "* Captures low-level features like edges and corners.\n",
        "\n",
        "**3. S2 ‚Äì Subsampling (Pooling) Layer**\n",
        "\n",
        "* Type: Average pooling (not max pooling)\n",
        "\n",
        "* Kernel size: 2√ó2\n",
        "\n",
        "* Stride: 2\n",
        "\n",
        "* Output size: 14√ó14√ó6\n",
        "\n",
        "* Activation: Tanh\n",
        "\n",
        "* Reduces spatial resolution, introduces translation invariance.\n",
        "\n",
        "**4. C3 ‚Äì Second Convolutional Layer**\n",
        "\n",
        "* Filter size: 5√ó5\n",
        "\n",
        "* Number of filters: 16\n",
        "\n",
        "* Output size: 10√ó10√ó16\n",
        "\n",
        "* Connectivity: Not all input maps are connected to all output maps (sparse connections to reduce computation and encourage feature diversity)\n",
        "\n",
        "* Learns more complex features by combining input maps in selective patterns.\n",
        "\n",
        "**5. S4 ‚Äì Subsampling Layer**\n",
        "\n",
        "* Same as S2\n",
        "\n",
        "* Output size: 5√ó5√ó16\n",
        "\n",
        "**6. C5 ‚Äì Fully Connected Convolution Layer**\n",
        "\n",
        "* Input size: 5√ó5√ó16 = 400\n",
        "\n",
        "* Filter size: 5√ó5 (entire input map)\n",
        "\n",
        "* Number of filters: 120\n",
        "\n",
        "* Output size: 1√ó1√ó120 (essentially a dense layer)\n",
        "\n",
        "* Activation: Tanh\n",
        "\n",
        "* Transitions from spatial to abstract representation for classification.\n",
        "\n",
        "**7. F6 ‚Äì Fully Connected Layer**\n",
        "\n",
        "* Input: 120\n",
        "\n",
        "* Output: 84\n",
        "\n",
        "* Activation: Tanh\n",
        "\n",
        "* Inspired by biological visual systems (e.g., the number of neurons in some human visual regions).\n",
        "\n",
        "**8. Output Layer**\n",
        "\n",
        "* Input: 84\n",
        "\n",
        "* Output: 10 (for digit classification 0‚Äì9)\n",
        "\n",
        "* Activation: Typically a softmax for classification\n",
        "\n",
        "**Reference**\n",
        "\n",
        "* LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324."
      ],
      "metadata": {
        "id": "kII0qX204-zR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: Compare and contrast AlexNet and VGGNet in terms of design principles, number of parameters, and performance. Highlight key innovations and limitations of each.**\n",
        "\n",
        "**Architecture Comparison**\n",
        "\n",
        "**AlexNet (2012)**\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "5 convolutional layers\n",
        "\n",
        "3 fully connected layers\n",
        "\n",
        "ReLU activation (first major use in CNNs)\n",
        "\n",
        "Local Response Normalization (LRN)\n",
        "\n",
        "Max pooling after some conv layers\n",
        "\n",
        "Dropout in fully connected layers\n",
        "\n",
        "**Design Highlights:**\n",
        "\n",
        "Introduced ReLU ‚Üí enabled faster training than sigmoid/tanh\n",
        "\n",
        "GPU parallelism: Split model across 2 GPUs due to hardware limits\n",
        "\n",
        "Dropout ‚Üí reduced overfitting in dense layers\n",
        "\n",
        "Trained on ImageNet (1.2M images)\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "Large fully connected layers ‚Üí most parameters come from here\n",
        "\n",
        "Irregular filter sizes (e.g., 11√ó11, 5√ó5)\n",
        "\n",
        "Model size is large for modest depth\n",
        "\n",
        "**VGGNet (2014)**\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "VGG-16: 13 convolutional + 3 fully connected layers\n",
        "\n",
        "All conv layers use 3√ó3 filters\n",
        "\n",
        "Max pooling every 2 or 3 conv layers\n",
        "\n",
        "ReLU activation\n",
        "\n",
        "No local response normalization (unlike AlexNet)\n",
        "\n",
        "**Design Highlights:**\n",
        "\n",
        "Deep but simple: Stacked many small filters instead of fewer large ones\n",
        "\n",
        "Demonstrated that depth improves performance significantly\n",
        "\n",
        "Used only 3√ó3 convs and 2√ó2 max pooling, making it clean and modular\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "Very high number of parameters (~138M) ‚Üí computationally expensive\n",
        "\n",
        "Not efficient for real-time or mobile applications\n",
        "\n",
        "Fully connected layers are again parameter-heavy\n",
        "\n",
        "**Number of Parameters**\n",
        "\n",
        "Model......................\tParameters (approx.)\n",
        "\n",
        "AlexNet....................\t~60 million\n",
        "\n",
        "VGG-16.....................\t~138 million\n",
        "\n",
        "VGG-19.....................\t~144 million\n",
        "\n",
        "**Performance (Top-5 Accuracy on ImageNet)**\n",
        "\n",
        "Model.....................\tTop-5 Accuracy\n",
        "\n",
        "AlexNet...................\t~83.6%\n",
        "\n",
        "VGG-16....................\t~92.7%\n",
        "\n",
        "VGG-19....................\t~92.8%\n",
        "\n",
        "\n",
        "**Key Innovations**\n",
        "\n",
        "**AlexNet**\n",
        "\n",
        "ReLU Activation: Introduced ReLU to CNNs, accelerating convergence.\n",
        "\n",
        "Dropout: Used dropout to combat overfitting in fully connected layers.\n",
        "\n",
        "GPU Training: Trained on two GPUs in parallel (split model), enabling deep learning on large datasets.\n",
        "\n",
        "Local Response Normalization (LRN): Introduced LRN for lateral inhibition (now rarely used).\n",
        "\n",
        "**VGGNet**\n",
        "\n",
        "Deep but Simple Design: Used a uniform 3√ó3 filter size throughout, demonstrating that depth improves performance if designed carefully.\n",
        "\n",
        "Stacked Small Filters: Multiple 3√ó3 convs approximate the receptive field of larger filters (e.g., 5√ó5), but with fewer parameters and more non-linearities.\n",
        "\n",
        "Modular Design: Easy to extend, which influenced later models like ResNet and EfficientNet.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "**AlexNet**\n",
        "\n",
        "Large Filters: Initial 11√ó11 and 5√ó5 filters are less efficient at capturing fine details.\n",
        "\n",
        "Shallow by Modern Standards: Only 8 layers; less expressive than deeper networks.\n",
        "\n",
        "Split Model for GPUs: Architecture was constrained by hardware limits (split across two GPUs).\n",
        "\n",
        "**VGGNet**\n",
        "\n",
        "Very Large Model Size: Over 500MB for VGG-16; not practical for deployment on resource-constrained devices.\n",
        "\n",
        "Training Time: Slow to train due to depth and high number of parameters.\n",
        "\n",
        "No Batch Normalization: Lacked techniques that later became standard (e.g., batch norm in ResNet)."
      ],
      "metadata": {
        "id": "gvlc8Q1-8Zyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is transfer learning in the context of image classification? Explain how it helps in reducing computational costs and improving model performance with limited data.**\n",
        "\n",
        "**In the Context of Image Classification**\n",
        "\n",
        "**In image classification, transfer learning involves:**\n",
        "\n",
        "Starting with a CNN pre-trained on a large dataset (e.g., ImageNet with 1.2M images and 1000 classes).\n",
        "\n",
        "Adapting it to a new task, such as classifying medical images, satellite imagery, or custom categories (e.g., cats vs. dogs).\n",
        "\n",
        "Either:\n",
        "\n",
        "Fine-tuning the whole model on the new dataset, or\n",
        "\n",
        "Freezing earlier layers and only training the final few layers (often the classification head).\n",
        "\n",
        "**How It Works: Two Main Strategies**\n",
        "\n",
        "1. Feature Extraction\n",
        "\n",
        "Freeze all convolutional layers (they act as a generic feature extractor).\n",
        "\n",
        "Replace the final classification layer (e.g., Dense(1000)) with one suited for your task (e.g., Dense(5) for 5 classes).\n",
        "\n",
        "Only train the final layer(s).\n",
        "\n",
        "2. Fine-tuning\n",
        "\n",
        "Unfreeze some of the later layers of the pre-trained model.\n",
        "\n",
        "Train them (often at a lower learning rate) to adapt to the new data.\n",
        "\n",
        "This improves performance when your dataset is moderately sized and similar in domain to the pretraining data.\n",
        "\n",
        "**Benefits of Transfer Learning**\n",
        "\n",
        "1. Reduced Computational Cost\n",
        "\n",
        "No need to train from scratch: Training a large CNN (e.g., ResNet, VGG) from scratch can take days or weeks.\n",
        "\n",
        "Transfer learning uses the heavy lifting already done by pretraining, saving time and compute.\n",
        "\n",
        "2. Better Performance with Limited Data\n",
        "\n",
        "Pre-trained models have already learned rich, general features (edges, shapes, textures).\n",
        "\n",
        "These features can be reused, especially in early layers, leading to higher accuracy even with small datasets (few thousand or even hundreds of images).\n",
        "\n",
        "3. Faster Convergence\n",
        "\n",
        "Training starts from an already-optimized set of weights.\n",
        "\n",
        "Fewer epochs are needed to reach high performance."
      ],
      "metadata": {
        "id": "z7ZTdjUM-xfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Describe the role of residual connections in ResNet architecture. How do they address the vanishing gradient problem in deep CNNs?**\n",
        "\n",
        "**Role of Residual Connections in ResNet**\n",
        "\n",
        "Residual connections‚Äîthe key innovation in ResNet (Residual Network)‚Äîare a simple yet powerful technique that allows very deep neural networks to be trained effectively. Introduced by He et al. in 2015 in the paper:\n",
        "\n",
        "\"Deep Residual Learning for Image Recognition\"\n",
        "üìÑ He, K., Zhang, X., Ren, S., & Sun, J. (2015)\n",
        "\n",
        "ResNet won the ImageNet 2015 challenge and enabled networks as deep as 152 layers to outperform shallower ones ‚Äî something previously difficult due to training challenges.\n",
        "\n",
        "What Are Residual Connections?\n",
        "\n",
        "A residual connection (or skip connection) bypasses one or more layers by directly adding the input of a layer to its output:\n",
        "\n",
        "Output = ùêπ(ùë•)+ ùë•\n",
        "\n",
        "Where:\n",
        "\n",
        "ùë•\n",
        "is the input,\n",
        "\n",
        "ùêπ\n",
        "(\n",
        "ùë•\n",
        ") is the residual mapping (the transformation the layers are trying to learn),\n",
        "\n",
        "The result is the element-wise addition of the input and the output of the residual block.\n",
        "\n",
        "**Purpose of Residual Connections**\n",
        "\n",
        "1. Easier Optimization of Deep Networks\n",
        "\n",
        "Without residual connections, very deep CNNs tend to saturate or degrade in performance ‚Äî accuracy gets worse as depth increases.\n",
        "\n",
        "Residual blocks allow the network to learn the \"difference\" (residual) from the identity mapping rather than the full transformation.\n",
        "\n",
        "If learning an identity mapping is optimal, the residual function can learn to output zeros, making the block effectively skip itself.\n",
        "\n",
        "2. Mitigating the Vanishing Gradient Problem\n",
        "\n",
        "In deep networks, during backpropagation:\n",
        "\n",
        "Gradients can become very small (vanish) as they propagate backward through many layers.\n",
        "\n",
        "This slows or prevents effective weight updates in early layers.\n",
        "\n",
        "With residual connections:\n",
        "\n",
        "The gradient has a shortcut path to flow directly backward through the identity connection.\n",
        "\n",
        "This helps preserve gradient strength, enabling effective training of very deep networks (50, 101, 152+ layers).\n",
        "\n",
        "Mathematically:\n",
        "Gradient flows through both:\n",
        "\n",
        "‚àÇ\n",
        "ùêπ\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "‚àÇ\n",
        "ùë•\n",
        "‚Äî through the normal layers\n",
        "\n",
        "‚àÇ\n",
        "ùë•\n",
        "‚àÇ\n",
        "ùë•\n",
        "= 1 ‚Äî through the identity shortcut\n",
        "\n",
        "This ensures that some gradient always flows, avoiding complete vanishing.\n",
        "\n",
        "3. Improved Generalization and Training Speed\n",
        "\n",
        "Residual connections make it easier for the network to converge during training.\n",
        "\n",
        "Deeper ResNets often generalize better because they can represent complex functions without increasing training error.\n",
        "\n",
        "Residual Block (Basic Unit of ResNet)\n",
        "\n",
        "A basic residual block looks like this:\n",
        "\n",
        "    Input ‚Üí [Conv ‚Üí BN ‚Üí ReLU ‚Üí Conv ‚Üí BN] ‚Üí Add(Input) ‚Üí ReLU ‚Üí Output\n"
      ],
      "metadata": {
        "id": "YfC1e6QxAF66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Implement the LeNet-5 architectures using Tensorflow or PyTorch to classify the MNIST dataset. Report the accuracy and training time.**\n"
      ],
      "metadata": {
        "id": "PcT-M5X8BW_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# LeNet-5 Model Definition\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = x.view(-1, 120)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Data preprocessing and loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(2),  # Convert 28x28 to 32x32 as expected by LeNet-5\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "model = LeNet5().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEdFn_ngBt6_",
        "outputId": "2a424828-34be-489d-ffd6-cbc395b75f62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:00<00:00, 56.1MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 1.49MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 13.2MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 9.66MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.3420\n",
            "Epoch [2/5], Loss: 0.0944\n",
            "Epoch [3/5], Loss: 0.0677\n",
            "Epoch [4/5], Loss: 0.0519\n",
            "Epoch [5/5], Loss: 0.0430\n",
            "\n",
            "Test Accuracy: 98.43%\n",
            "Training Time: 150.53 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Use a pre-trained VGG16 model (via transfer learning) on a small custom dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model. Include your code and result discussion.**\n",
        "\n",
        "Step-by-Step: Transfer Learning with VGG16\n",
        "\n",
        "Install and Import Dependencies\n",
        "\n",
        "    pip install torchvision\n",
        "\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torchvision import datasets, transforms, models\n",
        "    from torch.utils.data import DataLoader\n",
        "    import time\n",
        "    \n"
      ],
      "metadata": {
        "id": "Qdurb-IsB9n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the Dataset (Example: Flowers)\n",
        "\n",
        "# Transform: Resize and normalize (as VGG expects ImageNet input)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Download Oxford Flowers dataset (or use your own)\n",
        "train_dataset = datasets.Flowers102(root='./data', split='train', transform=transform, download=True)\n",
        "val_dataset = datasets.Flowers102(root='./data', split='val', transform=transform, download=True)\n",
        "test_dataset = datasets.Flowers102(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6cC9p2oCzwt",
        "outputId": "1e8b0313-46d5-419d-e14a-60a6e92b854e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345M/345M [00:09<00:00, 37.2MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 502/502 [00:00<00:00, 1.50MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.0k/15.0k [00:00<00:00, 24.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGG16 and Replace Classifier\n",
        "\n",
        "# Load pre-trained VGG16\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Freeze feature extractor\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the classifier\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(4096, 102),  # 102 flower classes\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "cpxVCVpDCtsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "training_time = time.time() - start_time\n"
      ],
      "metadata": {
        "id": "5yNOWF7UDJ5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Accuracy**"
      ],
      "metadata": {
        "id": "sDuN_uxZDRop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "val_acc = evaluate(val_loader)\n",
        "test_acc = evaluate(test_loader)\n",
        "\n",
        "print(f\"\\nValidation Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "KpUY2sgXDT0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Output (You May See Results Like):**"
      ],
      "metadata": {
        "id": "ckRLH8M1DW4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Epoch 1/5, Loss: 3.1152\n",
        "Epoch 2/5, Loss: 2.3524\n",
        "...\n",
        "Validation Accuracy: 74.85%\n",
        "Test Accuracy: 73.20%\n",
        "Training Time: 310.45 seconds\n"
      ],
      "metadata": {
        "id": "D3TVrgDsDXwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result Discussion**\n",
        "\n",
        "Observations:\n",
        "\n",
        "VGG16, even with a few training epochs and frozen base layers, achieves good accuracy on a small dataset.\n",
        "\n",
        "Fine-tuning only the classifier layers gives a strong starting point.\n",
        "\n",
        "You can unfreeze some deeper layers to further improve performance (with more training time and data)."
      ],
      "metadata": {
        "id": "sw9un4goDfbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a program to visualize the filters and feature maps of the first convolutional layer of AlexNet on an example input image.**\n",
        "\n",
        "Goal\n",
        "\n",
        "Load a pre-trained AlexNet model.\n",
        "\n",
        "Feed an input image into the network.\n",
        "\n",
        "Visualize:\n",
        "\n",
        "The filters (kernels) of the first convolutional layer.\n",
        "\n",
        "The feature maps (activations) produced by that layer.\n",
        "\n",
        "Prerequisites\n",
        "\n",
        "Install PyTorch and torchvision if not already:\n",
        "\n",
        "     pip install torch torchvision matplotlib\n"
      ],
      "metadata": {
        "id": "1MvYmbk1DmcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained AlexNet\n",
        "alexnet = models.alexnet(pretrained=True).to(device)\n",
        "alexnet.eval()\n",
        "\n",
        "# Load and preprocess an input image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
        "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
        "    )\n",
        "])\n",
        "\n",
        "# Load a sample image (replace with your own image path)\n",
        "image_path = 'sample.jpg'  # Ensure this image exists\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "# ================================\n",
        "# üîç 1. Visualize Filters of First Conv Layer\n",
        "# ================================\n",
        "\n",
        "def visualize_filters(layer, num_filters=16):\n",
        "    filters = layer.weight.data.cpu()\n",
        "    fig, axes = plt.subplots(1, num_filters, figsize=(20, 5))\n",
        "    for i in range(num_filters):\n",
        "        f = filters[i]\n",
        "        f = (f - f.min()) / (f.max() - f.min())  # Normalize\n",
        "        axes[i].imshow(f.permute(1, 2, 0))\n",
        "        axes[i].axis('off')\n",
        "    plt.suptitle(\"Filters from First Conv Layer\")\n",
        "    plt.show()\n",
        "\n",
        "first_conv = alexnet.features[0]\n",
        "visualize_filters(first_conv, num_filters=8)  # Show 8 filters\n",
        "\n",
        "# ================================\n",
        "# üîç 2. Visualize Feature Maps (Activations)\n",
        "# ================================\n",
        "\n",
        "# Hook to extract feature maps\n",
        "activation = {}\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    activation[\"conv1\"] = output.detach()\n",
        "\n",
        "# Register hook\n",
        "alexnet.features[0].register_forward_hook(hook_fn)\n",
        "\n",
        "# Forward pass\n",
        "_ = alexnet(input_tensor)\n",
        "\n",
        "# Visualize feature maps\n",
        "feature_maps = activation[\"conv1\"].squeeze().cpu()\n",
        "num_maps = 8  # Limit for visualization\n",
        "\n",
        "fig, axes = plt.subplots(1, num_maps, figsize=(20, 5))\n",
        "for i in range(num_maps):\n",
        "    axes[i].imshow(feature_maps[i], cmap='viridis')\n",
        "    axes[i].axis('off')\n",
        "plt.suptitle(\"Feature Maps from First Conv Layer\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DmsOXts9D6s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Train a GoogLeNet (Inception v1) or its variant using a standard dataset like CIFAR-10. Plot the training and validation accuracy over epochs and analyze overfitting or underfitting.**"
      ],
      "metadata": {
        "id": "K3VP2lVqD_h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Datasets and loaders\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load GoogLeNet (Inception v1)\n",
        "model = models.googlenet(pretrained=True)\n",
        "model.fc = nn.Linear(1024, 10)  # 10 classes in CIFAR-10\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, aux1, aux2 = model(images)\n",
        "        loss = criterion(outputs, labels) + 0.3 * criterion(aux1, labels) + 0.3 * criterion(aux2, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "    train_acc_history.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "    val_acc_history.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_acc_history, label='Train Accuracy')\n",
        "plt.plot(val_acc_history, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('GoogLeNet on CIFAR-10: Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QwGlWG8OEfiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working in a healthcare AI startup. Your team is tasked with developing a system that automatically classifies medical X-ray images into normal, pneumonia, and COVID-19. Due to limited labeled data, what approach would you suggest using among CNN architectures discussed (e.g., transfer learning with ResNet or Inception variants)? Justify your approach and outline a deployment strategy for production use.**\n",
        "\n"
      ],
      "metadata": {
        "id": "Rk8ljx8QEkWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Path to dataset (assumes structure: /train/class_name/, /val/class_name/)\n",
        "data_dir = \"./chest_xray_data\"\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "val_dir = os.path.join(data_dir, \"val\")\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Datasets and loaders\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load ResNet-50\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze early layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the classifier\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 3)  # 3 classes: Normal, Pneumonia, COVID-19\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "train_acc_list = []\n",
        "val_acc_list = []\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_acc = 100 * correct / total\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Acc = {train_acc:.2f}%, Val Acc = {val_acc:.2f}%\")\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_acc_list, label=\"Train Accuracy\")\n",
        "plt.plot(val_acc_list, label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4Jo_lZn1GEzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}